{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMuxIFg4GTydTFeuuRlvV0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oeam96/EasyTranscriptApp/blob/main/Transcript_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 1.  Runtime deps  (≈40 s on a fresh Colab VM)\n",
        "# ======================================================\n",
        "!apt-get update && apt-get install -y ffmpeg\n",
        "!pip install -q --upgrade \"gradio>=4\" \"faster-whisper>=1.0\" ffmpeg-python\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Rg0xMSksxbm6",
        "outputId": "1234f58e-8717-414e-9bcb-e88ca0bb1a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Mon Jul  7 12:22:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P0             29W /   70W |    1446MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C-ZKFTTTuPGa",
        "outputId": "e3186804-0949-40b9-f918-4b219c157b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6f14ae57af9a719f75.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6f14ae57af9a719f75.gradio.live\" width=\"100%\" height=\"1300\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7874 <> https://6f14ae57af9a719f75.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 1   Imports & helpers\n",
        "# ======================================================\n",
        "import os, subprocess, time, gc, tempfile, pathlib, re, shlex\n",
        "import gradio as gr, torch\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "# ---------- ffmpeg: any-format → AAC .m4a -------------------------------\n",
        "def convert_to_m4a(inp: str, audio_bitrate: str = \"192k\") -> str:\n",
        "    \"\"\"\n",
        "    Return path of an .m4a copy of *inp*.\n",
        "    Tries GPU video decode; falls back to CPU on failure.\n",
        "    \"\"\"\n",
        "    base, _ = os.path.splitext(inp)\n",
        "    out = base + \".m4a\"\n",
        "    ext_video = {\"mp4\", \"mov\", \"mkv\", \"avi\", \"flv\", \"webm\"}\n",
        "    use_gpu = inp.rsplit(\".\", 1)[-1].lower() in ext_video\n",
        "\n",
        "    cmd = [\"ffmpeg\", \"-y\"]\n",
        "    if use_gpu:\n",
        "        cmd += [\"-hwaccel\", \"cuda\", \"-c:v\", \"h264_cuvid\"]\n",
        "    cmd += [\"-i\", inp, \"-vn\", \"-c:a\", \"aac\", \"-b:a\", audio_bitrate, out]\n",
        "\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        if use_gpu:                          # retry once on CPU\n",
        "            print(\"GPU decode failed → retrying on CPU\")\n",
        "            return convert_to_m4a(inp, audio_bitrate)\n",
        "        else:\n",
        "            print(\"FFmpeg stderr:\\n\", e.stderr)\n",
        "            raise\n",
        "    return out\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# ---------- strip timestamps -------------------------------------------\n",
        "_PAT = [\n",
        "    re.compile(r\"^\\s*\\d+(?:\\.\\d+)?s\\s*-->\\s*\\d+(?:\\.\\d+)?s:\\s*(.*)$\"),\n",
        "    re.compile(r\"^\\s*\\d+(?:\\.\\d+)?s\\s*→\\s*\\d+(?:\\.\\d+)?s\\s*\\|\\s*(.*)$\"),\n",
        "]\n",
        "def _clean_line(l: str) -> str:\n",
        "    for p in _PAT:\n",
        "        m = p.match(l)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "    return l.strip()\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# ---------- Whisper model ----------------------------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL  = WhisperModel(\n",
        "    \"small\",\n",
        "    device=DEVICE,\n",
        "    compute_type=\"float16\" if DEVICE == \"cuda\" else \"float32\"\n",
        ")\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# ======================================================\n",
        "# 2   Streaming worker (always 6 outputs)\n",
        "# ======================================================\n",
        "def _out(status=\"\", elapsed=\"\", words=\"\", length=\"\", transcript=\"\", fpath=None):\n",
        "    \"\"\"Always return exactly six values.\"\"\"\n",
        "    return status, elapsed, words, length, transcript, fpath\n",
        "\n",
        "def transcribe_stream(audio_file: str):\n",
        "    yield _out(\"⏳ preparing input…\")\n",
        "\n",
        "    # -------- ensure .m4a ----------------------------------------------\n",
        "    if not audio_file.endswith(\".m4a\"):\n",
        "        yield _out(\"🚀 converting with FFmpeg…\")\n",
        "        try:\n",
        "            t_conv = time.perf_counter()\n",
        "            audio_file = convert_to_m4a(audio_file)\n",
        "            yield _out(f\"✅ FFmpeg done in {time.perf_counter()-t_conv:0.1f}s\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            yield _out(\"❌ FFmpeg failed\", transcript=f\"Conversion error:\\n{e}\")\n",
        "            return\n",
        "    else:\n",
        "        yield _out(\"🎧 audio already .m4a\")\n",
        "\n",
        "    # -------- Whisper streaming ---------------------------------------\n",
        "    t0 = time.perf_counter()\n",
        "    lines, last_end = [], 0\n",
        "    status = \"📝 decoding…\"\n",
        "\n",
        "    segs, _ = MODEL.transcribe(\n",
        "        audio_file,\n",
        "        beam_size=1,\n",
        "        language=\"es\",\n",
        "        best_of=5,\n",
        "        temperature=(0.0, 0.2, 0.4, 0.6),\n",
        "        compression_ratio_threshold=2.4,\n",
        "        repetition_penalty=1.1,\n",
        "        condition_on_previous_text=False,\n",
        "        vad_filter=True\n",
        "    )\n",
        "\n",
        "    for s in segs:\n",
        "        last_end = s.end\n",
        "        lines.append(f\"{s.start:7.2f}s → {s.end:7.2f}s | {s.text}\")\n",
        "        elapsed  = f\"{time.perf_counter() - t0:0.1f} s\"\n",
        "        words    = str(len(\" \".join(_clean_line(l) for l in lines).split()))\n",
        "        length_m = f\"{last_end/60:0.2f}\"\n",
        "        yield _out(status, elapsed, words, length_m, \"\\n\".join(lines))\n",
        "\n",
        "    # -------- create .txt ---------------------------------------------\n",
        "    clean = \" \".join(_clean_line(l) for l in lines)\n",
        "    tmp   = tempfile.NamedTemporaryFile(delete=False, suffix=\".txt\").name\n",
        "    pathlib.Path(tmp).write_text(clean, encoding=\"utf-8\")\n",
        "\n",
        "    elapsed  = f\"{time.perf_counter() - t0:0.1f} s\"\n",
        "    words    = str(len(clean.split()))\n",
        "    length_m = f\"{last_end/60:0.2f}\"\n",
        "    yield _out(\"🎉 finished\", elapsed, words, length_m, \"\\n\".join(lines), tmp)\n",
        "    gc.collect()\n",
        "\n",
        "# ======================================================\n",
        "# 3   GUI\n",
        "# ======================================================\n",
        "css_rule = \"\"\"\n",
        "* {font-family:'Segoe UI','Helvetica Neue',Arial,sans-serif !important;}\n",
        ".grp-box {border:1px solid var(--block-border-color); padding:8px; border-radius:4px;}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(title=\"Audio-to-Text\", theme=gr.themes.Monochrome(),\n",
        "               css=css_rule) as demo:\n",
        "\n",
        "    gr.Markdown(\"### 🎙️ Audio-to-Text Transcriber\")\n",
        "    gr.Markdown(\n",
        "        \"To create a Memory Aid Document visit: \"\n",
        "        \"[chatgpt Memory-Aid](https://chatgpt.com/g/g-68498babc37c8191bb25104819f9862e-ayudasmemoria-rdc)\"\n",
        "    )\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        file_in = gr.File(label=\"Upload audio / video\",\n",
        "                          file_types=[\"audio\", \"video\"], type=\"filepath\")\n",
        "\n",
        "        with gr.Group(elem_classes=\"grp-box\"):\n",
        "            with gr.Row(equal_height=True):\n",
        "                status_tx = gr.Textbox(label=\"Status\", lines=1, interactive=False)\n",
        "                time_tx   = gr.Textbox(label=\"Processing time\", lines=1, interactive=False)\n",
        "            with gr.Row(equal_height=True):\n",
        "                words_tx  = gr.Textbox(label=\"Words count\", lines=1, interactive=False)\n",
        "                len_tx    = gr.Textbox(label=\"Transcription length (min)\",\n",
        "                                       lines=1, interactive=False)\n",
        "\n",
        "    run_btn       = gr.Button(\"Transcribe ▶︎\", variant=\"primary\", size=\"lg\")\n",
        "    transcript_tx = gr.Textbox(label=\"Transcript\", lines=13, interactive=False)\n",
        "    download_bt   = gr.File(label=\"Download clean .txt\")\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=transcribe_stream,\n",
        "        inputs=file_in,\n",
        "        outputs=[status_tx, time_tx, words_tx, len_tx,\n",
        "                 transcript_tx, download_bt]     # exactly 6 outputs\n",
        "    )\n",
        "\n",
        "# ======================================================\n",
        "# 4   Launch\n",
        "# ======================================================\n",
        "demo.launch(share=True, inbrowser=True, height=1300, debug=True)"
      ]
    }
  ]
}